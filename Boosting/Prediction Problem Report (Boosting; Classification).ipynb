{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b1c13c80",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction Problem Report (Boosting; Classification)\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f043041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, LogisticRegressionCV, LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, cross_validate, \\\n",
    "GridSearchCV, RandomizedSearchCV, ParameterGrid, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier, \\\n",
    "VotingRegressor, VotingClassifier, StackingRegressor, StackingClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier, AdaBoostRegressor,AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, \\\n",
    "accuracy_score, precision_score, confusion_matrix, mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn import impute\n",
    "import ast\n",
    "import itertools as it\n",
    "from sklearn.tree import export_graphviz \n",
    "from six import StringIO\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/Users/kevin/Downloads/Northwestern University/Data Science/STAT_303-3/Prediction Problems/Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e86f05",
   "metadata": {},
   "source": [
    "## Step 0) Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06025554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_classification.csv')\n",
    "test = pd.read_csv('test_classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e561d3",
   "metadata": {},
   "source": [
    "## Step 1) Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe620d",
   "metadata": {},
   "source": [
    "### <font color = 'red'>Pre-processing training data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "0df2d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ten = train.iloc[:, :10]\n",
    "\n",
    "# Removing: ['id', 'host_location', 'host_neighbourhood']\n",
    "cleaned_ten = first_ten.drop(columns=['id', 'host_location', 'host_neighbourhood'])\n",
    "\n",
    "# Converting: ['host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_since']\n",
    "cleaned_ten['host_response_rate'] = pd.to_numeric(cleaned_ten['host_response_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_acceptance_rate'] = pd.to_numeric(cleaned_ten['host_acceptance_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_is_superhost'] = cleaned_ten['host_is_superhost'].map({'t': 1, 'f': 0})\n",
    "cleaned_ten['host_since'] = pd.to_datetime(cleaned_ten['host_since'])\n",
    "cleaned_ten['days_since_host'] = (pd.datetime.now() - cleaned_ten['host_since']).dt.days\n",
    "cleaned_ten = cleaned_ten.drop(columns=['host_since'])\n",
    "\n",
    "\n",
    "\n",
    "second_ten = train.iloc[:, 10:20]\n",
    "\n",
    "# Converting: ['host_has_profile_pic', 'neighbourhood_cleansed', 'host_identity_verified','latitude', 'longitude', 'property_type', 'room_type']\n",
    "cleaned_twenty = second_ten\n",
    "neighbourhood_counts = cleaned_twenty.neighbourhood_cleansed.value_counts()\n",
    "neighbourhoods_to_replace = neighbourhood_counts[neighbourhood_counts < 107].index.tolist()\n",
    "cleaned_twenty['neighbourhood_cleansed'] = cleaned_twenty['neighbourhood_cleansed'].replace(neighbourhoods_to_replace, 'Other')\n",
    "cleaned_twenty['num_verifications'] = cleaned_twenty['host_verifications'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['host_verifications'])\n",
    "cleaned_twenty['host_has_profile_pic'] = cleaned_twenty['host_has_profile_pic'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['host_identity_verified'] = cleaned_twenty['host_identity_verified'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['latitude'] = pd.to_numeric(cleaned_twenty['latitude'])\n",
    "cleaned_twenty['longitude'] = pd.to_numeric(cleaned_twenty['longitude'])\n",
    "cleaned_twenty['property_category'] = \"Entire property\"\n",
    "cleaned_twenty.loc[cleaned_twenty['property_type'].str.contains('room', case=False), 'property_category'] = 'Room'\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['property_type'])\n",
    "\n",
    "\n",
    "\n",
    "third_ten = train.iloc[:, 20:30]\n",
    "\n",
    "# Converting: ['bathrooms_text', 'price']\n",
    "third_ten['bathrooms_text'] = third_ten['bathrooms_text'].replace({\"Half-bath\": \"0.5\", \"Shared half-bath\": \"0.5\", \"Private half-bath\": \"0.5\"})\n",
    "third_ten['num_bathrooms'] = third_ten['bathrooms_text'].str.extract(r'(\\d+(\\.\\d+)?)')[0].astype(float)\n",
    "cleaned_third = third_ten.drop(columns=['bathrooms_text'])\n",
    "\n",
    "\n",
    "\n",
    "fourth_ten = train.iloc[:, 30:40]\n",
    "\n",
    "# Removing: ['first_review', 'last_review']\n",
    "cleaned_fourth = fourth_ten.drop(columns=['first_review'])\n",
    "cleaned_fourth = cleaned_fourth.drop(columns=['last_review'])\n",
    "\n",
    "# Converting: ['has_availability']\n",
    "cleaned_fourth['has_availability'] = cleaned_fourth['has_availability'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "fifth_ten = train.iloc[:, 40:50]\n",
    "fifth_ten\n",
    "\n",
    "# Converting: ['instant_bookable']\n",
    "cleaned_fifth = fifth_ten\n",
    "cleaned_fifth['instant_bookable'] = cleaned_fifth['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "last_three = train.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "38b7a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the cleaned datasets\n",
    "cleaned_train = pd.concat([cleaned_ten, cleaned_twenty, cleaned_third, cleaned_fourth, cleaned_fifth, last_three], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "ffcc415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "\n",
    "columns_with_missing = ['host_response_rate', 'host_acceptance_rate', 'beds', \\\n",
    "                        'num_bathrooms', 'review_scores_rating']\n",
    "\n",
    "cleaned_train['reviews_per_month'].fillna(cleaned_train['reviews_per_month'].mode()[0], inplace=True)\n",
    "cleaned_train['host_response_time'].fillna(cleaned_train['host_response_time'].mode()[0], inplace=True)\n",
    "\n",
    "# Computing the missing values of numeric variables using KNN\n",
    "\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=10)\n",
    "cleaned_train_imputed = knn_imputer.fit_transform(cleaned_train[columns_with_missing])\n",
    "cleaned_train_imputed_df = pd.DataFrame(cleaned_train_imputed, columns=columns_with_missing)\n",
    "cleaned_train[columns_with_missing] = cleaned_train_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "dce13813",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_removed = ['review_scores_communication','review_scores_cleanliness', 'number_of_reviews_l30d', \\\n",
    "                                'review_scores_accuracy', 'review_scores_value','review_scores_location', \\\n",
    "                                'review_scores_checkin', 'minimum_minimum_nights', 'maximum_minimum_nights', \\\n",
    "                                'minimum_maximum_nights', 'maximum_maximum_nights', 'availability_60', \\\n",
    "                                'availability_90', 'availability_365','calculated_host_listings_count',\n",
    "                                'calculated_host_listings_count_entire_homes', 'host_listings_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "1f62ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = cleaned_train.host_is_superhost\n",
    "X_train = cleaned_train.drop(\"host_is_superhost\", axis = 1).iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "41d9ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_non_redundant = X_train.copy()\n",
    "X_train_non_redundant.drop(columns = to_be_removed, inplace = True)\n",
    "X_train_non_redundant = pd.get_dummies(X_train_non_redundant, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d4b16",
   "metadata": {},
   "source": [
    "#### PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "f9dbb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2, include_bias = False)\n",
    "X_train_poly = poly.fit_transform(X_train_non_redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "cf5d8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_non_scaled_poly_df = pd.DataFrame(X_train_poly, columns = poly.get_feature_names_out(X_train_non_redundant.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45657d22",
   "metadata": {},
   "source": [
    "### <font color = 'red'>Pre-processing test data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "58d0753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ten = test.iloc[:, :10]\n",
    "\n",
    "# Removing: ['id', 'host_id', 'host_since', 'host_location', 'host_response_time', 'host_neighbourhood']\n",
    "cleaned_ten = first_ten.drop(columns=['host_id', 'host_location', 'host_neighbourhood'])\n",
    "\n",
    "# Converting: ['host_response_rate', 'host_acceptance_rate', 'host_is_superhost']\n",
    "cleaned_ten['host_response_rate'] = pd.to_numeric(cleaned_ten['host_response_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_acceptance_rate'] = pd.to_numeric(cleaned_ten['host_acceptance_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_since'] = pd.to_datetime(cleaned_ten['host_since'])\n",
    "cleaned_ten['days_since_host'] = (pd.datetime.now() - cleaned_ten['host_since']).dt.days\n",
    "cleaned_ten = cleaned_ten.drop(columns=['host_since'])\n",
    "\n",
    "\n",
    "\n",
    "second_ten = test.iloc[:, 10:20]\n",
    "\n",
    "# Converting: ['host_has_profile_pic', 'neighbourhood_cleansed', 'host_verifications', 'host_identity_verified','latitude', 'longitude', 'property_type', 'room_type']\n",
    "cleaned_twenty = second_ten\n",
    "neighbourhood_counts = cleaned_twenty.neighbourhood_cleansed.value_counts()\n",
    "neighbourhoods_to_replace = neighbourhood_counts[neighbourhood_counts < 64].index.tolist()\n",
    "cleaned_twenty['neighbourhood_cleansed'] = cleaned_twenty['neighbourhood_cleansed'].replace(neighbourhoods_to_replace, 'Other')\n",
    "cleaned_twenty['num_verifications'] = cleaned_twenty['host_verifications'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "cleaned_twenty = second_ten.drop(columns=['host_verifications'])\n",
    "cleaned_twenty['host_has_profile_pic'] = cleaned_twenty['host_has_profile_pic'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['host_identity_verified'] = cleaned_twenty['host_identity_verified'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['latitude'] = pd.to_numeric(cleaned_twenty['latitude'])\n",
    "cleaned_twenty['longitude'] = pd.to_numeric(cleaned_twenty['longitude'])\n",
    "cleaned_twenty['bathrooms_text'] = cleaned_twenty['bathrooms_text'].replace({\"Half-bath\": \"0.5\", \"Shared half-bath\": \"0.5\", \"Private half-bath\": \"0.5\"})\n",
    "cleaned_twenty['num_bathrooms'] = cleaned_twenty['bathrooms_text'].str.extract(r'(\\d+(\\.\\d+)?)')[0].astype(float)\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['bathrooms_text'])\n",
    "\n",
    "cleaned_twenty['property_category'] = \"Entire property\"\n",
    "cleaned_twenty.loc[cleaned_twenty['property_type'].str.contains('room', case=False), 'property_category'] = 'Room'\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['property_type'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "third_ten = test.iloc[:, 20:30]\n",
    "\n",
    "# Converting: ['has_availability']\n",
    "third_ten['has_availability'] = third_ten['has_availability'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fourth_ten = test.iloc[:, 30:40]\n",
    "\n",
    "# Removing: ['first_review', 'last_review']\n",
    "# Converting: []\n",
    "cleaned_fourth = fourth_ten.drop(columns=['first_review', 'last_review'])\n",
    "\n",
    "\n",
    "\n",
    "fifth_ten = test.iloc[:, 40:50]\n",
    "# Converting: ['instant_bookable']\n",
    "cleaned_fifth = fifth_ten\n",
    "cleaned_fifth['instant_bookable'] = cleaned_fifth['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "last_three = test.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "8fe3b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining test data\n",
    "cleaned_test = pd.concat([cleaned_ten, cleaned_twenty, third_ten, cleaned_fourth, cleaned_fifth, last_three], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "9f2a1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "\n",
    "columns_with_missing = ['host_response_rate', 'host_acceptance_rate', 'beds', \\\n",
    "                        'num_bathrooms', 'review_scores_rating']\n",
    "\n",
    "cleaned_test['reviews_per_month'].fillna(cleaned_test['reviews_per_month'].mode()[0], inplace=True)\n",
    "cleaned_test['host_response_time'].fillna(cleaned_test['host_response_time'].mode()[0], inplace=True)\n",
    "\n",
    "# Computing the missing values of numeric variables using KNN\n",
    "\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=10)\n",
    "cleaned_test_imputed = knn_imputer.fit_transform(cleaned_test[columns_with_missing])\n",
    "cleaned_test_imputed_df = pd.DataFrame(cleaned_test_imputed, columns=columns_with_missing)\n",
    "cleaned_test[columns_with_missing] = cleaned_test_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "d655994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_non_redundant = cleaned_test.drop(columns = to_be_removed)\n",
    "X_test_non_redundant = pd.get_dummies(X_test_non_redundant, drop_first = True)\n",
    "X_test_non_redundant = X_test_non_redundant.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "4cfadd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_test = PolynomialFeatures(2, include_bias = False)\n",
    "poly_test.fit(X_test_non_redundant)\n",
    "X_test_poly = poly_test.transform(X_test_non_redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "d528243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_non_scaled_poly_df = pd.DataFrame(X_test_poly, columns = poly_test.get_feature_names_out(X_test_non_redundant.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "ed67f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_columns = ['days_since_host host_total_listings_count', 'beds num_bathrooms', 'minimum_nights num_bathrooms', 'maximum_nights num_bathrooms', 'minimum_nights_avg_ntm num_bathrooms', 'maximum_nights_avg_ntm num_bathrooms']\n",
    "for column in reversed_columns:\n",
    "    predictors = column.split(' ')\n",
    "    old_column = predictors[1] + ' ' + predictors[0]\n",
    "    X_test_non_scaled_poly_df[column] = X_test_non_scaled_poly_df[old_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730eaefd",
   "metadata": {},
   "source": [
    "## Step 2) Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b39b7",
   "metadata": {},
   "source": [
    "### How many attempts did it take you to tune the model hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f50fd",
   "metadata": {},
   "source": [
    "### Which tuning method did you use (grid search / Bayes search / etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0da667",
   "metadata": {},
   "source": [
    "### What challenges did you face while tuning the hyperparameters, and what actions did you take to address those challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26daac",
   "metadata": {},
   "source": [
    "### How many hours did you spend on hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4abb9",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e10fcc",
   "metadata": {},
   "source": [
    "### <font color = red>Variable selection with Lasso</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "bdc8b9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train_poly)\n",
    "X_train_scaled = scaler.transform(X_train_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "deaf7587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken =  7  minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "Cs = np.logspace(-1,-5, 50)\n",
    "model_cv = LogisticRegressionCV(Cs = Cs, cv=5, penalty='l1', solver = 'saga', random_state=1)\n",
    "model_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "5720fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = {}\n",
    "for i in range(len(model_cv.coef_[0])):\n",
    "    coefficients[poly.get_feature_names_out()[i]] = model_cv.coef_[0][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "d3f993f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.Series(data = coefficients)\n",
    "non_zero_coefficients = coefficients[coefficients != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40285a",
   "metadata": {},
   "source": [
    "### Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "0792be89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n",
      "Time taken =  449.3603666861852 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [500,1000,2000,5000]\n",
    "grid['learning_rate'] = [0.01,0.1,1.0,2.0,5.0]\n",
    "grid['max_depth'] = [5,10,16,24,32,40]\n",
    "grid['subsample'] = [0.25,0.5,0.75,1.0]\n",
    "# define the evaluation procedure\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# define the grid search procedure\n",
    "randomized_search = RandomizedSearchCV(estimator=model, param_distributions=grid, n_iter = 150, n_jobs=-1, cv=cv, verbose = True, scoring = 'accuracy')\n",
    "# execute the grid search\n",
    "randomized_result = randomized_search.fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)\n",
    "print(\"Time taken = \", (time.time() - start_time)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "9943dc8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.895921 using {'subsample': 0.75, 'n_estimators': 5000, 'max_depth': 5, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (randomized_result.best_score_, randomized_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fee304",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_model = GradientBoostingClassifier(random_state=1,max_depth=5,learning_rate=0.1,subsample=0.75,\n",
    "                          n_estimators=5000).fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec91c4a",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "fa6542f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n",
      "{'scale_pos_weight': 1.5, 'reg_lambda': 0.001, 'n_estimators': 3750, 'max_depth': 7, 'learning_rate': 0.2, 'gamma': 0.005} 0.8846735686464451\n",
      "Time taken =  35.687345600128175  minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "param_grid = {'n_estimators':[500, 1000, 2250, 3750, 5000],\n",
    "                'max_depth': [2,4,7,10,16,24],\n",
    "              'learning_rate': [0.01,0.2,0.5,1.0,2.0],\n",
    "               'gamma': [0.005,0.1,0.175,0.3, 0.5],\n",
    "               'reg_lambda':[0,0.01,0.005,0.001],\n",
    "                'scale_pos_weight':[1.0,1.25,1.5]#Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances).\n",
    "             }\n",
    "\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "optimal_params = RandomizedSearchCV(estimator=xgb.XGBClassifier(objective = 'binary:logistic',random_state=1,use_label_encoder=False),\n",
    "                             param_distributions = param_grid, n_iter = 150,\n",
    "                             scoring = 'accuracy',\n",
    "                             verbose = 1,\n",
    "                             n_jobs=-1,\n",
    "                             cv = cv)\n",
    "optimal_params.fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)\n",
    "print(optimal_params.best_params_,optimal_params.best_score_)\n",
    "print(\"Time taken = \", (time.time()-start_time)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "128d3af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective = 'binary:logistic',random_state=1,gamma=0.005,learning_rate = 0.2,max_depth=7,\n",
    "                              n_estimators = 3700,reg_lambda = 0.001,scale_pos_weight=1.5).fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa1d8d3",
   "metadata": {},
   "source": [
    "### <font color>CatBoost</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "232ccd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Time taken =  340.8866595784823  minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "param_grid = {'max_depth': [4,6,8,10],\n",
    "              'num_leaves': [20, 31, 40, 60],\n",
    "              'learning_rate': [0.02, 0.05, 0.08],\n",
    "               'reg_lambda':[0.01,2.5,5,8],\n",
    "                'n_estimators':[1000, 1250, 1500],\n",
    "                'subsample': [0.5, 0.75, 1.0],\n",
    "             'colsample_bylevel': [0.25, 0.5, 0.75, 1.0]}\n",
    "\n",
    "cv = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "optimal_params = RandomizedSearchCV(estimator=CatBoostClassifier(random_state=1, verbose=False, \n",
    "                            grow_policy='Lossguide'),                                                       \n",
    "                             param_distributions = param_grid, n_iter = 200,\n",
    "                             verbose = 1,random_state = 1, scoring='accuracy',\n",
    "                             n_jobs=-1,\n",
    "                             cv = cv)\n",
    "optimal_params.fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)\n",
    "print(\"Time taken = \", (time.time()-start_time)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "05ac631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subsample': 0.5, 'reg_lambda': 0.01, 'num_leaves': 31, 'n_estimators': 1000, 'max_depth': 8, 'learning_rate': 0.02, 'colsample_bylevel': 1.0} 0.892909527557466\n"
     ]
    }
   ],
   "source": [
    "print(optimal_params.best_params_,optimal_params.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e104de7",
   "metadata": {},
   "source": [
    "## Step 3) Developing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "24359082",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_model = GradientBoostingClassifier(random_state=1,max_depth=5,learning_rate=0.1,subsample=0.75,\n",
    "                          n_estimators=5000).fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "848a2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective = 'binary:logistic',random_state=1,gamma=0.3,learning_rate = 0.2,max_depth=5,\n",
    "                              n_estimators = 5000,reg_lambda = 0,scale_pos_weight=1.5).fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "4c21b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "untuned_catboost = CatBoostClassifier(random_state=1, verbose=0).fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "a83e7692",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_catboost = CatBoostClassifier(grow_policy='Lossguide', random_state=1, verbose=0, subsample=0.5, reg_lambda=0.01, num_leaves=20,\n",
    "                                  n_estimators=1250, max_depth=6, learning_rate=0.08, colsample_bylevel=0.5).fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1e7527",
   "metadata": {},
   "source": [
    "### Ensembling with (Hard Voting) (Performed better than Stacking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "6c30fcbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('tuned',\n",
       "                              <catboost.core.CatBoostClassifier object at 0x7fad40685910>),\n",
       "                             ('untuned',\n",
       "                              <catboost.core.CatBoostClassifier object at 0x7fad7833df10>)])"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_model_catboost = VotingClassifier(estimators=[('tuned',catboost_model),('untuned',untuned_catboost)])\n",
    "hard_model_catboost.fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)\n",
    "#hard_model.fit(X_train_non_scaled_poly_df.loc[:, better_non_zero],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "ab82567f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hard_ensembled_pred = hard_model.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index])\n",
    "hard_ensembled_pred_catboost = hard_model_catboost.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528a9c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_model = VotingClassifier(estimators=[('xg',xgb_model),('gradient',gradient_model)])\n",
    "# hard_model.fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)\n",
    "hard_model.fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_ensemble_pred = hard_model.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eb1ba3",
   "metadata": {},
   "source": [
    "### Making the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "95e488bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_catboost_pred = tuned_catboost.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "624f6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "untuned_catboost_pred = untuned_catboost.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "f1c78200",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_pred = gradient_model.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "58136b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = xgb_model.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e77a66",
   "metadata": {},
   "source": [
    "### Ensemble of ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "dce6b9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('cat',\n",
       "                              VotingClassifier(estimators=[('tuned',\n",
       "                                                            <catboost.core.CatBoostClassifier object at 0x7fad40685910>),\n",
       "                                                           ('untuned',\n",
       "                                                            <catboost.core.CatBoostClassifier object at 0x7fad7833df10>)])),\n",
       "                             ('others',\n",
       "                              VotingClassifier(estimators=[('cat',\n",
       "                                                            VotingClassifier(estimators=[('tuned',\n",
       "                                                                                          <catboost.core.CatBoostClassifier object at 0x7fad40685910>),\n",
       "                                                                                         ('un...\n",
       "                                                                                                        max_cat_threshold=None,\n",
       "                                                                                                        max_cat_to_onehot=None,\n",
       "                                                                                                        max_delta_step=None,\n",
       "                                                                                                        max_depth=5,\n",
       "                                                                                                        max_leaves=None,\n",
       "                                                                                                        min_child_weight=None,\n",
       "                                                                                                        missing=nan,\n",
       "                                                                                                        monotone_constraints=None,\n",
       "                                                                                                        multi_strategy=None,\n",
       "                                                                                                        n_estimators=5000,\n",
       "                                                                                                        n_jobs=None,\n",
       "                                                                                                        num_parallel_tree=None,\n",
       "                                                                                                        random_state=1, ...)),\n",
       "                                                                                         ('gradient',\n",
       "                                                                                          GradientBoostingClassifier(max_depth=5,\n",
       "                                                                                                                     n_estimators=5000,\n",
       "                                                                                                                     random_state=1,\n",
       "                                                                                                                     subsample=0.75))]))]))])"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_hard_model = VotingClassifier(estimators=[('cat',hard_model_catboost),('others',hard_model)])\n",
    "# hard_model.fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)\n",
    "meta_hard_model.fit(X_train_non_scaled_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "ccf2de4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41335740072202165"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_hard_model.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "ae020e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column in the dataset called `predicted`\n",
    "\n",
    "cleaned_test.insert(1, \"predicted\", xgb_pred)\n",
    "to_submit = cleaned_test.iloc[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d6954",
   "metadata": {},
   "source": [
    "## Step 4) Ad-hoc steps for further improving model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0aeaa",
   "metadata": {},
   "source": [
    "#### <font color = 'red'>Matching known host_id's</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "b261262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "known = train.loc[:, ['id', 'host_id','host_is_superhost']]\n",
    "known['host_is_superhost'] = known['host_is_superhost'].map({'t': 1, 'f': 0})\n",
    "\n",
    "not_known = test.loc[:, ['id', 'host_id']]\n",
    "\n",
    "overlapping_host_ids = set(known['host_id']).intersection(set(not_known['host_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "ad83109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for host_id in overlapping_host_ids:\n",
    "    host_is_superhost = known.loc[known['host_id'] == host_id, 'host_is_superhost'].iloc[0]\n",
    "    not_known.loc[not_known['host_id'] == host_id, 'host_is_superhost'] = host_is_superhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "8a3a3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(to_submit, not_known[['id', 'host_is_superhost']], on='id', how='left')\n",
    "merged_df.host_is_superhost = merged_df.host_is_superhost.fillna(merged_df.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "a3cb8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = merged_df.iloc[:, [0,2]].rename(columns = {'host_is_superhost':'predicted'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c5d42",
   "metadata": {},
   "source": [
    "## Step 5) Exporting the predictions in the format required to submit on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "31202106",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission.to_csv('Ensembling Classification - Trial 24 (Tuned XGB on selected predictors).csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6effd99d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
