{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b1c13c80",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction Problem Report (RF; Classification)\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6f043041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, \\\n",
    "cross_validate, GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.ensemble import BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, \\\n",
    "accuracy_score, precision_score, confusion_matrix, mean_squared_error, r2_score, mean_absolute_error\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn import impute\n",
    "import ast\n",
    "import itertools as it\n",
    "\n",
    "from sklearn.tree import export_graphviz \n",
    "from six import StringIO\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "import time as time\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/Users/kevin/Downloads/Northwestern University/Data Science/STAT_303-3/Prediction Problems/Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e86f05",
   "metadata": {},
   "source": [
    "## Step 0) Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "06025554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_classification.csv')\n",
    "test = pd.read_csv('test_classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e561d3",
   "metadata": {},
   "source": [
    "## Step 1) Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe620d",
   "metadata": {},
   "source": [
    "### <font color = 'red'>Pre-processing training data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0df2d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ten = train.iloc[:, :10]\n",
    "\n",
    "# Removing: ['id', 'host_location', 'host_neighbourhood']\n",
    "cleaned_ten = first_ten.drop(columns=['id', 'host_location', 'host_neighbourhood'])\n",
    "\n",
    "# Converting: ['host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_since']\n",
    "cleaned_ten['host_response_rate'] = pd.to_numeric(cleaned_ten['host_response_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_acceptance_rate'] = pd.to_numeric(cleaned_ten['host_acceptance_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_is_superhost'] = cleaned_ten['host_is_superhost'].map({'t': 1, 'f': 0})\n",
    "cleaned_ten['host_since'] = pd.to_datetime(cleaned_ten['host_since'])\n",
    "cleaned_ten['days_since_host'] = (pd.datetime.now() - cleaned_ten['host_since']).dt.days\n",
    "cleaned_ten = cleaned_ten.drop(columns=['host_since'])\n",
    "\n",
    "\n",
    "\n",
    "second_ten = train.iloc[:, 10:20]\n",
    "\n",
    "# Converting: ['host_has_profile_pic', 'neighbourhood_cleansed', 'host_identity_verified','latitude', 'longitude', 'property_type', 'room_type']\n",
    "cleaned_twenty = second_ten\n",
    "neighbourhood_counts = cleaned_twenty.neighbourhood_cleansed.value_counts()\n",
    "neighbourhoods_to_replace = neighbourhood_counts[neighbourhood_counts < 107].index.tolist()\n",
    "cleaned_twenty['neighbourhood_cleansed'] = cleaned_twenty['neighbourhood_cleansed'].replace(neighbourhoods_to_replace, 'Other')\n",
    "cleaned_twenty['num_verifications'] = cleaned_twenty['host_verifications'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['host_verifications'])\n",
    "cleaned_twenty['host_has_profile_pic'] = cleaned_twenty['host_has_profile_pic'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['host_identity_verified'] = cleaned_twenty['host_identity_verified'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['latitude'] = pd.to_numeric(cleaned_twenty['latitude'])\n",
    "cleaned_twenty['longitude'] = pd.to_numeric(cleaned_twenty['longitude'])\n",
    "cleaned_twenty['property_category'] = \"Entire property\"\n",
    "cleaned_twenty.loc[cleaned_twenty['property_type'].str.contains('room', case=False), 'property_category'] = 'Room'\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['property_type'])\n",
    "\n",
    "\n",
    "\n",
    "third_ten = train.iloc[:, 20:30]\n",
    "\n",
    "# Converting: ['bathrooms_text', 'price']\n",
    "third_ten['bathrooms_text'] = third_ten['bathrooms_text'].replace({\"Half-bath\": \"0.5\", \"Shared half-bath\": \"0.5\", \"Private half-bath\": \"0.5\"})\n",
    "third_ten['num_bathrooms'] = third_ten['bathrooms_text'].str.extract(r'(\\d+(\\.\\d+)?)')[0].astype(float)\n",
    "cleaned_third = third_ten.drop(columns=['bathrooms_text'])\n",
    "\n",
    "\n",
    "\n",
    "fourth_ten = train.iloc[:, 30:40]\n",
    "\n",
    "# Removing: ['first_review', 'last_review']\n",
    "cleaned_fourth = fourth_ten.drop(columns=['first_review'])\n",
    "cleaned_fourth = cleaned_fourth.drop(columns=['last_review'])\n",
    "\n",
    "# Converting: ['has_availability']\n",
    "cleaned_fourth['has_availability'] = cleaned_fourth['has_availability'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "fifth_ten = train.iloc[:, 40:50]\n",
    "fifth_ten\n",
    "\n",
    "# Converting: ['instant_bookable']\n",
    "cleaned_fifth = fifth_ten\n",
    "cleaned_fifth['instant_bookable'] = cleaned_fifth['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "last_three = train.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "38b7a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the cleaned datasets\n",
    "cleaned_train = pd.concat([cleaned_ten, cleaned_twenty, cleaned_third, cleaned_fourth, cleaned_fifth, last_three], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ffcc415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "\n",
    "columns_with_missing = ['host_response_rate', 'host_acceptance_rate', 'beds', \\\n",
    "                        'num_bathrooms', 'review_scores_rating']\n",
    "\n",
    "cleaned_train['reviews_per_month'].fillna(cleaned_train['reviews_per_month'].mode()[0], inplace=True)\n",
    "cleaned_train['host_response_time'].fillna(cleaned_train['host_response_time'].mode()[0], inplace=True)\n",
    "\n",
    "# Computing the missing values of numeric variables using KNN\n",
    "\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=10)\n",
    "cleaned_train_imputed = knn_imputer.fit_transform(cleaned_train[columns_with_missing])\n",
    "cleaned_train_imputed_df = pd.DataFrame(cleaned_train_imputed, columns=columns_with_missing)\n",
    "cleaned_train[columns_with_missing] = cleaned_train_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dce13813",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_removed = ['review_scores_communication','review_scores_cleanliness', 'number_of_reviews_l30d', \\\n",
    "                                'review_scores_accuracy', 'review_scores_value','review_scores_location', \\\n",
    "                                'review_scores_checkin', 'minimum_minimum_nights', 'maximum_minimum_nights', \\\n",
    "                                'minimum_maximum_nights', 'maximum_maximum_nights', 'availability_60', \\\n",
    "                                'availability_90', 'availability_365','calculated_host_listings_count',\n",
    "                                'calculated_host_listings_count_entire_homes', 'host_listings_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1f62ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = cleaned_train.host_is_superhost\n",
    "X_train = cleaned_train.drop(\"host_is_superhost\", axis = 1).iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "41d9ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_non_redundant = X_train.copy()\n",
    "X_train_non_redundant.drop(columns = to_be_removed, inplace = True)\n",
    "X_train_non_redundant = pd.get_dummies(X_train_non_redundant, drop_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d4b16",
   "metadata": {},
   "source": [
    "#### PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f9dbb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2, interaction_only = True, include_bias = False)\n",
    "X_train_poly = poly.fit_transform(X_train_non_redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cf5d8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_non_scaled_poly_df = pd.DataFrame(X_train_poly, columns = poly.get_feature_names_out(X_train_non_redundant.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45657d22",
   "metadata": {},
   "source": [
    "### <font color = 'red'>Pre-processing test data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "58d0753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ten = test.iloc[:, :10]\n",
    "\n",
    "# Removing: ['id', 'host_id', 'host_since', 'host_location', 'host_response_time', 'host_neighbourhood']\n",
    "cleaned_ten = first_ten.drop(columns=['host_id', 'host_location', 'host_neighbourhood'])\n",
    "\n",
    "# Converting: ['host_response_rate', 'host_acceptance_rate', 'host_is_superhost']\n",
    "cleaned_ten['host_response_rate'] = pd.to_numeric(cleaned_ten['host_response_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_acceptance_rate'] = pd.to_numeric(cleaned_ten['host_acceptance_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_since'] = pd.to_datetime(cleaned_ten['host_since'])\n",
    "cleaned_ten['days_since_host'] = (pd.datetime.now() - cleaned_ten['host_since']).dt.days\n",
    "cleaned_ten = cleaned_ten.drop(columns=['host_since'])\n",
    "\n",
    "\n",
    "\n",
    "second_ten = test.iloc[:, 10:20]\n",
    "\n",
    "# Converting: ['host_has_profile_pic', 'neighbourhood_cleansed', 'host_verifications', 'host_identity_verified','latitude', 'longitude', 'property_type', 'room_type']\n",
    "cleaned_twenty = second_ten\n",
    "neighbourhood_counts = cleaned_twenty.neighbourhood_cleansed.value_counts()\n",
    "neighbourhoods_to_replace = neighbourhood_counts[neighbourhood_counts < 64].index.tolist()\n",
    "cleaned_twenty['neighbourhood_cleansed'] = cleaned_twenty['neighbourhood_cleansed'].replace(neighbourhoods_to_replace, 'Other')\n",
    "cleaned_twenty['num_verifications'] = cleaned_twenty['host_verifications'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "cleaned_twenty = second_ten.drop(columns=['host_verifications'])\n",
    "cleaned_twenty['host_has_profile_pic'] = cleaned_twenty['host_has_profile_pic'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['host_identity_verified'] = cleaned_twenty['host_identity_verified'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['latitude'] = pd.to_numeric(cleaned_twenty['latitude'])\n",
    "cleaned_twenty['longitude'] = pd.to_numeric(cleaned_twenty['longitude'])\n",
    "cleaned_twenty['bathrooms_text'] = cleaned_twenty['bathrooms_text'].replace({\"Half-bath\": \"0.5\", \"Shared half-bath\": \"0.5\", \"Private half-bath\": \"0.5\"})\n",
    "cleaned_twenty['num_bathrooms'] = cleaned_twenty['bathrooms_text'].str.extract(r'(\\d+(\\.\\d+)?)')[0].astype(float)\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['bathrooms_text'])\n",
    "\n",
    "cleaned_twenty['property_category'] = \"Entire property\"\n",
    "cleaned_twenty.loc[cleaned_twenty['property_type'].str.contains('room', case=False), 'property_category'] = 'Room'\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['property_type'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "third_ten = test.iloc[:, 20:30]\n",
    "\n",
    "# Converting: ['has_availability']\n",
    "third_ten['has_availability'] = third_ten['has_availability'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fourth_ten = test.iloc[:, 30:40]\n",
    "\n",
    "# Removing: ['first_review', 'last_review']\n",
    "# Converting: []\n",
    "cleaned_fourth = fourth_ten.drop(columns=['first_review', 'last_review'])\n",
    "\n",
    "\n",
    "\n",
    "fifth_ten = test.iloc[:, 40:50]\n",
    "# Converting: ['instant_bookable']\n",
    "cleaned_fifth = fifth_ten\n",
    "cleaned_fifth['instant_bookable'] = cleaned_fifth['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "last_three = test.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8fe3b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining test data\n",
    "cleaned_test = pd.concat([cleaned_ten, cleaned_twenty, third_ten, cleaned_fourth, cleaned_fifth, last_three], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9f2a1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "\n",
    "columns_with_missing = ['host_response_rate', 'host_acceptance_rate', 'beds', \\\n",
    "                        'num_bathrooms', 'review_scores_rating']\n",
    "\n",
    "cleaned_test['reviews_per_month'].fillna(cleaned_test['reviews_per_month'].mode()[0], inplace=True)\n",
    "cleaned_test['host_response_time'].fillna(cleaned_test['host_response_time'].mode()[0], inplace=True)\n",
    "\n",
    "# Computing the missing values of numeric variables using KNN\n",
    "\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=10)\n",
    "cleaned_test_imputed = knn_imputer.fit_transform(cleaned_test[columns_with_missing])\n",
    "cleaned_test_imputed_df = pd.DataFrame(cleaned_test_imputed, columns=columns_with_missing)\n",
    "cleaned_test[columns_with_missing] = cleaned_test_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d655994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_non_redundant = cleaned_test.drop(columns = to_be_removed)\n",
    "X_test_non_redundant = pd.get_dummies(X_test_non_redundant, drop_first = True)\n",
    "X_test_non_redundant = X_test_non_redundant.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4cfadd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_test = PolynomialFeatures(2, include_bias = False)\n",
    "poly_test.fit(X_test_non_redundant)\n",
    "X_test_poly = poly_test.transform(X_test_non_redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d528243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_non_scaled_poly_df = pd.DataFrame(X_test_poly, columns = poly_test.get_feature_names_out(X_test_non_redundant.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730eaefd",
   "metadata": {},
   "source": [
    "## Step 2) Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b39b7",
   "metadata": {},
   "source": [
    "### How many attempts did it take you to tune the model hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27090b",
   "metadata": {},
   "source": [
    "The first time that I tuned the model hyperparameters, I was able to achieve a classification accuracy of over 92% on Kaggle. I tuned the model hyperparameters using all of the predictors in the dataset after applying PolynomialFeatures with order 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f50fd",
   "metadata": {},
   "source": [
    "### Which tuning method did you use (grid search / Bayes search / etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ea3666",
   "metadata": {},
   "source": [
    "To tune the random forest model, I used a method similar to grid search: I tested each possible hyperparameter combination using a `for` loop and selected the one that resulted in the highest classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0da667",
   "metadata": {},
   "source": [
    "### What challenges did you face while tuning the hyperparameters, and what actions did you take to address those challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8149e",
   "metadata": {},
   "source": [
    "The main challenge that I faced was that the optimal hyperparameter values sometimes fluctated, which resulted in different accuracy scores on Kaggle (though they were all above the 92% threshold). Since I was worried about the possibility of the accuracy score dipping below 92% when this assignment will be evaluated, I decided to use `random_state=1` to stabilize the hyperparameter values and the resulting accuracy score.\n",
    "\n",
    "Another challenge that I faced was that some of the predictor names in my test dataset after utilizing PolynomialFeatures were the reverse of their names in my train dataset (e.g. 'days_since_host host_total_listings_count' vs 'host_total_listings_count days_since_host'). To address this issue, I created a `for` loop to ensure that all of the predictor names in my test dataset matched those in the train dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f26daac",
   "metadata": {},
   "source": [
    "### How many hours did you spend on hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8149d3",
   "metadata": {},
   "source": [
    "I spent about 30 minutes writing the code for the hyperparameter tuning process. The code only required 3 to 5 minutes to finish running."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4abb9",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "efccf837",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results = pd.DataFrame(columns = ['Threshold', 'Parameters', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a9938028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken =  3.0391077677408855  minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "params = {'n_estimators': [300],\n",
    "          'max_features': [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.6, 1.0]}\n",
    "\n",
    "param_list=list(it.product(*(params[Name] for Name in params)))\n",
    "thresholds = np.arange(0.01, 1.0, 0.01)\n",
    "    \n",
    "for pr in param_list:\n",
    "    model = RandomForestClassifier(random_state=1, oob_score=True, n_estimators=pr[0],max_features=pr[1], \\\n",
    "                                   n_jobs=-1).fit(X_train_non_scaled_poly_df, y_train)\n",
    "    \n",
    "    oob_prob = model.oob_decision_function_[:, 1]\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        oob_pred = (oob_prob >= threshold).astype(int)\n",
    "        oob_accuracy = accuracy_score(y_train, oob_pred)\n",
    "        analysis_results = analysis_results.append({'Threshold': threshold, 'Parameters': pr, 'Accuracy': oob_accuracy}, ignore_index=True)\n",
    "    \n",
    "    \n",
    "end_time = time.time()\n",
    "print(\"time taken = \", (end_time-start_time)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc357d",
   "metadata": {},
   "source": [
    "### Optimal hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "33497b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Threshold           0.49\n",
       "Parameters    (300, 0.3)\n",
       "Accuracy        0.883464\n",
       "Name: 543, dtype: object"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analysis_results.sort_values(by='Accuracy', ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce8e55",
   "metadata": {},
   "source": [
    "**The optimal value of `max_features` is 0.3 and the optimal decision threshold probability is 0.49.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e104de7",
   "metadata": {},
   "source": [
    "## Step 3) Developing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a6462944",
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_columns = ['days_since_host host_total_listings_count', 'beds num_bathrooms', 'minimum_nights num_bathrooms', 'maximum_nights num_bathrooms', 'minimum_nights_avg_ntm num_bathrooms', 'maximum_nights_avg_ntm num_bathrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "efb4710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in reversed_columns:\n",
    "    predictors = column.split(' ')\n",
    "    old_column = predictors[1] + ' ' + predictors[0]\n",
    "    X_test_non_scaled_poly_df[column] = X_test_non_scaled_poly_df[old_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bdc0befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = RandomForestClassifier(random_state=1, oob_score=True, n_estimators=500,\n",
    "                               max_features=0.3, n_jobs=-1).fit(X_train_non_scaled_poly_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "95e488bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tuned_model.predict_proba(X_test_non_scaled_poly_df.loc[:, X_train_non_scaled_poly_df.columns])\n",
    "predicted_class = y_pred[:,1] > 0.49\n",
    "test_pred = predicted_class.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ae020e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column in the dataset called `predicted`\n",
    "\n",
    "cleaned_test.insert(1, \"predicted\", test_pred)\n",
    "to_submit = cleaned_test.iloc[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d6954",
   "metadata": {},
   "source": [
    "## Step 4) Ad-hoc steps for further improving model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0aeaa",
   "metadata": {},
   "source": [
    "#### <font color = 'red'>Matching known host_id's</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b261262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "known = train.loc[:, ['id', 'host_id','host_is_superhost']]\n",
    "known['host_is_superhost'] = known['host_is_superhost'].map({'t': 1, 'f': 0})\n",
    "\n",
    "not_known = test.loc[:, ['id', 'host_id']]\n",
    "\n",
    "overlapping_host_ids = set(known['host_id']).intersection(set(not_known['host_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ad83109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for host_id in overlapping_host_ids:\n",
    "    host_is_superhost = known.loc[known['host_id'] == host_id, 'host_is_superhost'].iloc[0]\n",
    "    not_known.loc[not_known['host_id'] == host_id, 'host_is_superhost'] = host_is_superhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8a3a3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(to_submit, not_known[['id', 'host_is_superhost']], on='id', how='left')\n",
    "merged_df.host_is_superhost = merged_df.host_is_superhost.fillna(merged_df.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a3cb8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = merged_df.iloc[:, [0,2]].rename(columns = {'host_is_superhost':'predicted'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c5d42",
   "metadata": {},
   "source": [
    "## Step 5) Exporting the predictions in the format required to submit on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "31202106",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission.to_csv('Random Forest Classification - Final Submission.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
