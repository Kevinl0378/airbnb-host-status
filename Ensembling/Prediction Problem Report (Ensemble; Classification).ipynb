{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b1c13c80",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Prediction Problem Report (Ensemble; Classification)\"\n",
    "format: \n",
    "  html:\n",
    "    toc: true\n",
    "    toc-title: Contents\n",
    "    toc-depth: 4\n",
    "    code-fold: show\n",
    "    self-contained: true\n",
    "    html-math-method: mathml \n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "6f043041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, LogisticRegressionCV, LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, cross_validate, \\\n",
    "GridSearchCV, RandomizedSearchCV, ParameterGrid, KFold, StratifiedKFold, RepeatedKFold, RepeatedStratifiedKFold\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor,BaggingClassifier,RandomForestRegressor,RandomForestClassifier, \\\n",
    "VotingRegressor, VotingClassifier, StackingRegressor, StackingClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier, AdaBoostRegressor,AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, \\\n",
    "accuracy_score, precision_score, confusion_matrix, mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from sklearn import impute\n",
    "import ast\n",
    "import itertools as it\n",
    "from sklearn.tree import export_graphviz \n",
    "from six import StringIO\n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "os.getcwd()\n",
    "os.chdir('/Users/kevin/Downloads/Northwestern University/Data Science/STAT_303-3/Prediction Problems/Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e86f05",
   "metadata": {},
   "source": [
    "## Step 0) Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "06025554",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_classification.csv')\n",
    "test = pd.read_csv('test_classification.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e561d3",
   "metadata": {},
   "source": [
    "## Step 1) Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfe620d",
   "metadata": {},
   "source": [
    "### <font color = 'red'>Pre-processing training data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "0df2d2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ten = train.iloc[:, :10]\n",
    "\n",
    "# Removing: ['id', 'host_location', 'host_neighbourhood']\n",
    "cleaned_ten = first_ten.drop(columns=['id', 'host_location', 'host_neighbourhood'])\n",
    "\n",
    "# Converting: ['host_response_rate', 'host_acceptance_rate', 'host_is_superhost', 'host_since']\n",
    "cleaned_ten['host_response_rate'] = pd.to_numeric(cleaned_ten['host_response_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_acceptance_rate'] = pd.to_numeric(cleaned_ten['host_acceptance_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_is_superhost'] = cleaned_ten['host_is_superhost'].map({'t': 1, 'f': 0})\n",
    "cleaned_ten['host_since'] = pd.to_datetime(cleaned_ten['host_since'])\n",
    "cleaned_ten['days_since_host'] = (pd.datetime.now() - cleaned_ten['host_since']).dt.days\n",
    "cleaned_ten = cleaned_ten.drop(columns=['host_since'])\n",
    "\n",
    "\n",
    "\n",
    "second_ten = train.iloc[:, 10:20]\n",
    "\n",
    "# Converting: ['host_has_profile_pic', 'neighbourhood_cleansed', 'host_identity_verified','latitude', 'longitude', 'property_type', 'room_type']\n",
    "cleaned_twenty = second_ten\n",
    "neighbourhood_counts = cleaned_twenty.neighbourhood_cleansed.value_counts()\n",
    "neighbourhoods_to_replace = neighbourhood_counts[neighbourhood_counts < 107].index.tolist()\n",
    "cleaned_twenty['neighbourhood_cleansed'] = cleaned_twenty['neighbourhood_cleansed'].replace(neighbourhoods_to_replace, 'Other')\n",
    "cleaned_twenty['num_verifications'] = cleaned_twenty['host_verifications'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['host_verifications'])\n",
    "cleaned_twenty['host_has_profile_pic'] = cleaned_twenty['host_has_profile_pic'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['host_identity_verified'] = cleaned_twenty['host_identity_verified'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['latitude'] = pd.to_numeric(cleaned_twenty['latitude'])\n",
    "cleaned_twenty['longitude'] = pd.to_numeric(cleaned_twenty['longitude'])\n",
    "cleaned_twenty['property_category'] = \"Entire property\"\n",
    "cleaned_twenty.loc[cleaned_twenty['property_type'].str.contains('room', case=False), 'property_category'] = 'Room'\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['property_type'])\n",
    "\n",
    "\n",
    "\n",
    "third_ten = train.iloc[:, 20:30]\n",
    "\n",
    "# Converting: ['bathrooms_text', 'price']\n",
    "third_ten['bathrooms_text'] = third_ten['bathrooms_text'].replace({\"Half-bath\": \"0.5\", \"Shared half-bath\": \"0.5\", \"Private half-bath\": \"0.5\"})\n",
    "third_ten['num_bathrooms'] = third_ten['bathrooms_text'].str.extract(r'(\\d+(\\.\\d+)?)')[0].astype(float)\n",
    "cleaned_third = third_ten.drop(columns=['bathrooms_text'])\n",
    "\n",
    "\n",
    "\n",
    "fourth_ten = train.iloc[:, 30:40]\n",
    "\n",
    "# Removing: ['first_review', 'last_review']\n",
    "cleaned_fourth = fourth_ten.drop(columns=['first_review'])\n",
    "cleaned_fourth = cleaned_fourth.drop(columns=['last_review'])\n",
    "\n",
    "# Converting: ['has_availability']\n",
    "cleaned_fourth['has_availability'] = cleaned_fourth['has_availability'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "fifth_ten = train.iloc[:, 40:50]\n",
    "fifth_ten\n",
    "\n",
    "# Converting: ['instant_bookable']\n",
    "cleaned_fifth = fifth_ten\n",
    "cleaned_fifth['instant_bookable'] = cleaned_fifth['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "last_three = train.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "38b7a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the cleaned datasets\n",
    "cleaned_train = pd.concat([cleaned_ten, cleaned_twenty, cleaned_third, cleaned_fourth, cleaned_fifth, last_three], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "ffcc415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "\n",
    "columns_with_missing = ['num_bathrooms', 'reviews_per_month', 'host_is_superhost', \n",
    "                        'review_scores_rating', 'host_response_rate', \n",
    "                        'host_acceptance_rate', 'beds', 'review_scores_communication', \n",
    "                        'review_scores_cleanliness', 'review_scores_accuracy', \n",
    "                        'review_scores_value', 'review_scores_location', 'review_scores_checkin']\n",
    "\n",
    "cleaned_train['reviews_per_month'].fillna(cleaned_train['reviews_per_month'].mode()[0], inplace=True)\n",
    "cleaned_train['host_response_time'].fillna(cleaned_train['host_response_time'].mode()[0], inplace=True)\n",
    "\n",
    "# Computing the missing values of numeric variables using KNN\n",
    "\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=10)\n",
    "cleaned_train_imputed = knn_imputer.fit_transform(cleaned_train[columns_with_missing])\n",
    "cleaned_train_imputed_df = pd.DataFrame(cleaned_train_imputed, columns=columns_with_missing)\n",
    "cleaned_train[columns_with_missing] = cleaned_train_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "1f62ec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = cleaned_train.host_is_superhost\n",
    "X_train = cleaned_train.drop(\"host_is_superhost\", axis = 1).iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "227f5c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_logged = ['reviews_per_month','accommodates','beds', 'host_total_listings_count', 'minimum_nights', 'maximum_nights']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "8fef7732",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_logged_zero = ['number_of_reviews_ltm', 'number_of_reviews', 'num_bathrooms', \\\n",
    "                     'calculated_host_listings_count_private_rooms','calculated_host_listings_count_shared_rooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "a6872160",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in to_be_logged:\n",
    "    X_train[column] = np.log(X_train[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "2cfcb2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in to_be_logged_zero:\n",
    "    X_train[column] = np.log(1 + X_train[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "ba24ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_onehot = pd.get_dummies(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "c49fab27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 1\n",
    "for x in list(X_train_onehot.isnull().sum().sort_values().values):\n",
    "    if x != 0:\n",
    "        raise Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "6ca917df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 2\n",
    "for x in list((X_train_onehot == -np.inf).sum().sort_values().values):\n",
    "    if x != 0:\n",
    "        raise Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5d4b16",
   "metadata": {},
   "source": [
    "#### PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "f9dbb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_redundant = PolynomialFeatures(2, include_bias = False)\n",
    "X_train_redundant_poly = poly_redundant.fit_transform(X_train_onehot)\n",
    "X_train_redundant_poly_df = pd.DataFrame(X_train_redundant_poly, columns = poly_redundant.get_feature_names_out(X_train_onehot.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45657d22",
   "metadata": {},
   "source": [
    "### <font color = 'red'>Pre-processing test data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "58d0753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_ten = test.iloc[:, :10]\n",
    "\n",
    "# Removing: ['id', 'host_id', 'host_since', 'host_location', 'host_response_time', 'host_neighbourhood']\n",
    "cleaned_ten = first_ten.drop(columns=['host_id', 'host_location', 'host_neighbourhood'])\n",
    "\n",
    "# Converting: ['host_response_rate', 'host_acceptance_rate', 'host_is_superhost']\n",
    "cleaned_ten['host_response_rate'] = pd.to_numeric(cleaned_ten['host_response_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_acceptance_rate'] = pd.to_numeric(cleaned_ten['host_acceptance_rate'].str.strip('%')) / 100\n",
    "cleaned_ten['host_since'] = pd.to_datetime(cleaned_ten['host_since'])\n",
    "cleaned_ten['days_since_host'] = (pd.datetime.now() - cleaned_ten['host_since']).dt.days\n",
    "cleaned_ten = cleaned_ten.drop(columns=['host_since'])\n",
    "\n",
    "\n",
    "\n",
    "second_ten = test.iloc[:, 10:20]\n",
    "\n",
    "# Converting: ['host_has_profile_pic', 'neighbourhood_cleansed', 'host_verifications', 'host_identity_verified','latitude', 'longitude', 'property_type', 'room_type']\n",
    "cleaned_twenty = second_ten\n",
    "neighbourhood_counts = cleaned_twenty.neighbourhood_cleansed.value_counts()\n",
    "neighbourhoods_to_replace = neighbourhood_counts[neighbourhood_counts < 64].index.tolist()\n",
    "cleaned_twenty['neighbourhood_cleansed'] = cleaned_twenty['neighbourhood_cleansed'].replace(neighbourhoods_to_replace, 'Other')\n",
    "cleaned_twenty['num_verifications'] = cleaned_twenty['host_verifications'].apply(lambda x: len(ast.literal_eval(x)))\n",
    "cleaned_twenty = second_ten.drop(columns=['host_verifications'])\n",
    "cleaned_twenty['host_has_profile_pic'] = cleaned_twenty['host_has_profile_pic'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['host_identity_verified'] = cleaned_twenty['host_identity_verified'].map({'t': 1, 'f': 0})\n",
    "cleaned_twenty['latitude'] = pd.to_numeric(cleaned_twenty['latitude'])\n",
    "cleaned_twenty['longitude'] = pd.to_numeric(cleaned_twenty['longitude'])\n",
    "cleaned_twenty['bathrooms_text'] = cleaned_twenty['bathrooms_text'].replace({\"Half-bath\": \"0.5\", \"Shared half-bath\": \"0.5\", \"Private half-bath\": \"0.5\"})\n",
    "cleaned_twenty['num_bathrooms'] = cleaned_twenty['bathrooms_text'].str.extract(r'(\\d+(\\.\\d+)?)')[0].astype(float)\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['bathrooms_text'])\n",
    "\n",
    "cleaned_twenty['property_category'] = \"Entire property\"\n",
    "cleaned_twenty.loc[cleaned_twenty['property_type'].str.contains('room', case=False), 'property_category'] = 'Room'\n",
    "cleaned_twenty = cleaned_twenty.drop(columns=['property_type'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "third_ten = test.iloc[:, 20:30]\n",
    "\n",
    "# Converting: ['has_availability']\n",
    "third_ten['has_availability'] = third_ten['has_availability'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fourth_ten = test.iloc[:, 30:40]\n",
    "\n",
    "# Removing: ['first_review', 'last_review']\n",
    "# Converting: []\n",
    "cleaned_fourth = fourth_ten.drop(columns=['first_review', 'last_review'])\n",
    "\n",
    "\n",
    "\n",
    "fifth_ten = test.iloc[:, 40:50]\n",
    "# Converting: ['instant_bookable']\n",
    "cleaned_fifth = fifth_ten\n",
    "cleaned_fifth['instant_bookable'] = cleaned_fifth['instant_bookable'].map({'t': 1, 'f': 0})\n",
    "\n",
    "\n",
    "\n",
    "last_three = test.iloc[:, 50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "8fe3b513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining test data\n",
    "cleaned_test = pd.concat([cleaned_ten, cleaned_twenty, third_ten, cleaned_fourth, cleaned_fifth, last_three], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "9f2a1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "\n",
    "columns_with_missing = ['num_bathrooms', 'reviews_per_month', \n",
    "                        'review_scores_rating', 'host_response_rate', \n",
    "                        'host_acceptance_rate', 'beds', 'review_scores_communication', \n",
    "                        'review_scores_cleanliness', 'review_scores_accuracy', \n",
    "                        'review_scores_value', 'review_scores_location', 'review_scores_checkin', 'number_of_reviews_ltm']\n",
    "\n",
    "#cleaned_test['reviews_per_month'].fillna(cleaned_test['reviews_per_month'].mode()[0], inplace=True)\n",
    "cleaned_test['host_response_time'].fillna(cleaned_test['host_response_time'].mode()[0], inplace=True)\n",
    "\n",
    "# Computing the missing values of numeric variables using KNN\n",
    "\n",
    "knn_imputer = impute.KNNImputer(n_neighbors=10)\n",
    "cleaned_test_imputed = knn_imputer.fit_transform(cleaned_test[columns_with_missing])\n",
    "cleaned_test_imputed_df = pd.DataFrame(cleaned_test_imputed, columns=columns_with_missing)\n",
    "cleaned_test[columns_with_missing] = cleaned_test_imputed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b2d78b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_logged = ['reviews_per_month','accommodates','beds', 'host_total_listings_count', 'minimum_nights', 'maximum_nights']\n",
    "\n",
    "to_be_logged_zero = ['number_of_reviews_ltm', 'number_of_reviews', 'num_bathrooms', \\\n",
    "                     'calculated_host_listings_count_private_rooms','calculated_host_listings_count_shared_rooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "c4b2ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in to_be_logged:\n",
    "    cleaned_test[column] = np.log(cleaned_test[column], where = cleaned_test[column] > 0)\n",
    "    \n",
    "for column in to_be_logged_zero:\n",
    "    cleaned_test[column] = np.log(1 + cleaned_test[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "b75ac117",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_redundant = cleaned_test\n",
    "X_test_redundant = pd.get_dummies(X_test_redundant)\n",
    "X_test_redundant = X_test_redundant.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a8a13d",
   "metadata": {},
   "source": [
    "#### PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "42e8a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_test = PolynomialFeatures(2, include_bias = False)\n",
    "poly_test.fit(X_test_redundant)\n",
    "X_test_poly = poly_test.transform(X_test_redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "3557e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_non_scaled_poly_df = pd.DataFrame(X_test_poly, columns = poly_test.get_feature_names_out(X_test_redundant.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "c9469886",
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_columns = ['beds num_bathrooms']\n",
    "\n",
    "for column in reversed_columns:\n",
    "    predictors = column.split(' ')\n",
    "    old_column = predictors[1] + ' ' + predictors[0]\n",
    "    X_test_non_scaled_poly_df[column] = X_test_non_scaled_poly_df[old_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730eaefd",
   "metadata": {},
   "source": [
    "## Step 2) Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b39b7",
   "metadata": {},
   "source": [
    "### How many attempts did it take you to tune the model hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63776c0d",
   "metadata": {},
   "source": [
    "It took me 28 attempts to tune the model hyperparameters. During these attempts, I tried tuning a variety of different models, including bagging, random forest, and boosting, as well as multiple different ensembles of the models using both soft and hard voting. In the end, I was able to reach the 93.5% threshold using a soft voting ensemble with an untuned CatBoost, a tuned XGBoost, and a tuned GradientBoost as the base models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffab6f2b",
   "metadata": {},
   "source": [
    "### Which tuning method did you use (grid search / Bayes search / etc.)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a27654",
   "metadata": {},
   "source": [
    "I used the `RandomizedSearchCV` tuning method to tune the GradientBoost model. For the XGBoost model, I used the `GridSearchCV` tuning method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893aa4f1",
   "metadata": {},
   "source": [
    "### What challenges did you face while tuning the hyperparameters, and what actions did you take to address those challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3c5b1e",
   "metadata": {},
   "source": [
    "The main challenge that I faced was that no matter how much effort I put into the tuning process, the classification score on Kaggle would plateau at 93.2%. This was after I had undergone several tuning rounds for multiple different boosting models. In order to address this challenge, I decided to review the entirety of my code to determine if there was another aspect of the prediction problem that I was doing wrong. As I reviewed the pre-processing stage, I realized that I did not log-transform any of the predictors, despite the fact that some were skewed. Once I corrected this oversight, I retrained the boosting models on the newly selected predictors. Even though this did not immediately push me over the threshold, by using trial-and-error with the `VotingClassifier` function, I was able to find a combination of base boosting models that got me over the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976b022",
   "metadata": {},
   "source": [
    "### How many hours did you spend on hyperparameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5455ab25",
   "metadata": {},
   "source": [
    "In total, I spent about 3 days on hyperparameter tuning. Some of my earlier attempts at tuning boosting models took more than 3 hours to finish, so I would leave them running overnight. However, the tuning process for the base models that helped me reach the threshold were: 2 hours and 40 minutes for the GradientBoost model and 22 minutes for the XGBoost model. In addition, the hyperparameter tuning process for variable selection using Lasso took about 15 minutes to finish. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549c98d1",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343aeb2d",
   "metadata": {},
   "source": [
    "### <font color=blue> Variable Selection with Lasso</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "7c3b8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train_redundant_poly_df)\n",
    "X_train_scaled = scaler.transform(X_train_redundant_poly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "1ee092d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken =  14  minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "Cs = np.logspace(-1,-3,30)\n",
    "model_cv = LogisticRegressionCV(Cs = Cs, cv=5, penalty='l1', solver = 'saga', random_state=1)\n",
    "model_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Time taken = \", round((time.time()-start_time)/60), \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "d3f70711",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = {}\n",
    "for i in range(len(model_cv.coef_[0])):\n",
    "    coefficients[poly_redundant.get_feature_names_out()[i]] = model_cv.coef_[0][i]\n",
    "coefficients = pd.Series(data = coefficients)\n",
    "non_zero_coefficients = coefficients[coefficients != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a03a9c",
   "metadata": {},
   "source": [
    "### <font color = blue>Gradient Boost</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3190823f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Time taken =  194.54656725327175 minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "grid = dict()\n",
    "grid['n_estimators'] = [100,200,500,1000]\n",
    "grid['learning_rate'] = [0.0001, 0.001, 0.01,0.1, 1.0]\n",
    "grid['max_depth'] = [5,10,16,24,32,40]\n",
    "grid['subsample'] = [0.25,0.5,0.75,1.0]\n",
    "# define the evaluation procedure\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "# define the grid search procedure\n",
    "randomized_search = RandomizedSearchCV(estimator=model, param_distributions=grid, n_jobs=-1, cv=cv, n_iter=100, verbose=True, scoring='accuracy')\n",
    "# execute the grid search\n",
    "randomized_result = randomized_search.fit(X_train_redundant_poly_df.loc[:, non_zero_coefficients.index],y_train)\n",
    "print(\"Time taken = \", (time.time() - start_time)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4987aff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.903757 using {'subsample': 0.75, 'n_estimators': 500, 'max_depth': 10, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f using %s\" % (randomized_result.best_score_, randomized_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9b8766",
   "metadata": {},
   "source": [
    "### <font color = blue>XG Boost</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8b8d9bf6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 432 candidates, totalling 2160 fits\n",
      "{'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 700, 'reg_lambda': 0.01, 'scale_pos_weight': 1.278846} 0.9003424754293556\n",
      "Time taken =  39.94512161413829  minutes\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "param_grid = {'n_estimators':[300,500,700,1000],\n",
    "                'max_depth': [2,4,7,10],\n",
    "              'learning_rate': [0.01,0.1,0.5],\n",
    "               'gamma': [0.1,0.25,0.5],\n",
    "               'reg_lambda':[0,0.001,0.01],\n",
    "                'scale_pos_weight':[1.278846]\n",
    "             }\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n",
    "optimal_params = GridSearchCV(estimator=xgb.XGBClassifier(objective = 'binary:logistic',random_state=1,\n",
    "                                                         use_label_encoder=False),\n",
    "                             param_grid = param_grid,\n",
    "                             scoring = 'accuracy',\n",
    "                             verbose = 1,\n",
    "                             n_jobs=-1,\n",
    "                             cv = cv)\n",
    "optimal_params.fit(X_train_redundant_poly_df.loc[:, non_zero_coefficients.index],y_train)\n",
    "print(optimal_params.best_params_,optimal_params.best_score_)\n",
    "print(\"Time taken = \", (time.time()-start_time)/60, \" minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc357d",
   "metadata": {},
   "source": [
    "### Optimal hyperparameter values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ce8e55",
   "metadata": {},
   "source": [
    "**For the GradientBoost model, the optimal value of `subsample` is 0.75, the optimal `n_estimators` is 500, the optimal value of `max_depth` is 10, and the optimal learning rate is 0.1.**\n",
    "\n",
    "**For the XGBoost model, the optimal value of `gamma` is 0.1, the optimal learning rate is 0.1, the optimal value of `max_depth` is 4, the optimal value of `n_estimators` is 700, the optimal value of `reg_lambda` is 0.01, and the optimal value of `scale_pos_weight` is 1.278846.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "1c2ce613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal value of the lasso regularization hyperparameter is: 0.04520353656360243\n"
     ]
    }
   ],
   "source": [
    "print('The optimal value of the lasso regularization hyperparameter is:', model_cv.C_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e104de7",
   "metadata": {},
   "source": [
    "## Step 3) Developing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0187882",
   "metadata": {},
   "source": [
    "**CatBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "35ac35c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "untuned_catboost = CatBoostClassifier(random_state=1, verbose=0).fit(X_train_redundant_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "b995a6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "untuned_test_pred = untuned_catboost.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e18ad",
   "metadata": {},
   "source": [
    "**XG Boost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "07be0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(objective = 'binary:logistic',random_state=1,gamma=0.1,learning_rate = 0.1,max_depth=4,\n",
    "                              n_estimators = 700,reg_lambda = 0.01,scale_pos_weight=1.278846).fit(X_train_redundant_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "e188f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_test_pred = xgb_model.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9f48f",
   "metadata": {},
   "source": [
    "**Gradient Boost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "75fefb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_model = GradientBoostingClassifier(random_state=1,max_depth=10,learning_rate=0.1,subsample=0.75,\n",
    "                          n_estimators=500).fit(X_train_redundant_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "54e37c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_test_pred = gradient_model.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e606b98e",
   "metadata": {},
   "source": [
    "### <font color = blue>Ensembling using soft voting</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "a3e7081b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('untuned catboost',\n",
       "                              <catboost.core.CatBoostClassifier object at 0x7fc179b11100>),\n",
       "                             ('gb',\n",
       "                              GradientBoostingClassifier(max_depth=10,\n",
       "                                                         n_estimators=500,\n",
       "                                                         random_state=1,\n",
       "                                                         subsample=0.75)),\n",
       "                             ('xgb',\n",
       "                              XGBClassifier(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None, device=None,\n",
       "                                            early...\n",
       "                                            grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            interaction_constraints=None,\n",
       "                                            learning_rate=0.1, max_bin=None,\n",
       "                                            max_cat_threshold=None,\n",
       "                                            max_cat_to_onehot=None,\n",
       "                                            max_delta_step=None, max_depth=4,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            multi_strategy=None,\n",
       "                                            n_estimators=700, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            random_state=1, ...))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_model = VotingClassifier(estimators=[('untuned catboost',untuned_catboost),('gb',gradient_model),\n",
    "                                              ('xgb',xgb_model)], voting = 'soft')\n",
    "\n",
    "ensemble_model.fit(X_train_redundant_poly_df.loc[:, non_zero_coefficients.index],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "49c2e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_test = ensemble_model.predict(X_test_non_scaled_poly_df.loc[:, non_zero_coefficients.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6670abfa",
   "metadata": {},
   "source": [
    "### Inserting the prediction columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "a4ea7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column in the dataset called `predicted`\n",
    "\n",
    "cleaned_test.insert(1, \"predicted\", en_test)\n",
    "to_submit = cleaned_test.iloc[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d6954",
   "metadata": {},
   "source": [
    "## Step 4) Ad-hoc steps for further improving model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f0aeaa",
   "metadata": {},
   "source": [
    "#### <font color = 'red'>Matching known host_id's</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "b261262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "known = train.loc[:, ['id', 'host_id','host_is_superhost']]\n",
    "known['host_is_superhost'] = known['host_is_superhost'].map({'t': 1, 'f': 0})\n",
    "\n",
    "not_known = test.loc[:, ['id', 'host_id']]\n",
    "\n",
    "overlapping_host_ids = set(known['host_id']).intersection(set(not_known['host_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "ad83109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for host_id in overlapping_host_ids:\n",
    "    host_is_superhost = known.loc[known['host_id'] == host_id, 'host_is_superhost'].iloc[0]\n",
    "    not_known.loc[not_known['host_id'] == host_id, 'host_is_superhost'] = host_is_superhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "8a3a3a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(to_submit, not_known[['id', 'host_is_superhost']], on='id', how='left')\n",
    "merged_df.host_is_superhost = merged_df.host_is_superhost.fillna(merged_df.predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "a3cb8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission = merged_df.iloc[:, [0,2]].rename(columns = {'host_is_superhost':'predicted'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c5d42",
   "metadata": {},
   "source": [
    "## Step 5) Exporting the predictions in the format required to submit on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "31202106",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission.to_csv('Ensembling Classification - Confirmation.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
